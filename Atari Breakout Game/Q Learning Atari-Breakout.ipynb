{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "import random as rand\n",
    "env = gym.make('Breakout-ram-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting a continuouss state into a discrete state\n",
    "MAX_STATES = 10**1\n",
    "GAMMA = 0.7   #discount factor\n",
    "ALPHA = 0.001 #Learning rate\n",
    "\n",
    "#q table disctionary lookup(like weights) gets state with highest reward?\n",
    "#q table disctionary lookup(like weights) gets state with highest reward?\n",
    "def q_states_dict(q):\n",
    "    a,b = max(q.items(), key=lambda k: k[1])\n",
    "    return a,b\n",
    "# our inputs\n",
    "def create_bins():\n",
    "    bins = np.zeros((1,10))\n",
    "    bins = np.linspace(-1,256,10)\n",
    "    return bins\n",
    "#digitize bins since we are converting a continuous state to a discrete state\n",
    "\n",
    "def assign_bins(observation,bins):\n",
    "    env_state = np.zeros(1) #creates a single array to store the 4 states\n",
    "    env_state = np.digitize(observation,bins)\n",
    "    return env_state\n",
    "\n",
    "def get_state_as_string(env_state):\n",
    "    env_string = ''.join(str(int(e)) for e in env_state)\n",
    "    return env_string\n",
    "\n",
    "def get_all_states_string():\n",
    "    env_states = []\n",
    "    for i in range(MAX_STATES):\n",
    "        env_states.append(str(i).zfill(8))\n",
    "    return env_states\n",
    "\n",
    "def initialise_q():\n",
    "    Q = {}\n",
    "    all_states = get_all_states_string()\n",
    "    for state in all_states:\n",
    "        Q[state] = {}\n",
    "        for action in range(env.action_space.n):\n",
    "            Q[state][action] = 0 #set rewards\n",
    "    return Q\n",
    "\n",
    "def add_state_to_table(new_state,table):\n",
    "    n_value = {}\n",
    "    n_value[new_state] = {}\n",
    "    for action in range(env.action_space.n):\n",
    "        n_value[new_state][action] = 0\n",
    "    table.update(n_value)\n",
    "    \n",
    "def play_a_game(bins,Q,eps):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    count = 0\n",
    "    new_states_created = 0\n",
    "    total_reward = 0\n",
    "    state = get_state_as_string(assign_bins(observation,bins))\n",
    "    y = Q.get(state)\n",
    "    if y == None:\n",
    "        add_state_to_table(state,Q)\n",
    "        new_states_created+=1\n",
    "    while not done:\n",
    "        #print(\"state\",state)\n",
    "        count+=1\n",
    "        init_random = np.random.uniform(0,1)\n",
    "        if init_random < eps:\n",
    "            act = env.action_space.sample() #pick a random action\n",
    "        else:\n",
    "            act,_ = q_states_dict(Q[state])\n",
    "        observation,reward,done,_ = env.step(act)\n",
    "        total_reward+=reward\n",
    "        #print(\"total_reward\", total_reward)\n",
    "        if count >= 20 and total_reward < 18:\n",
    "            reward = -5\n",
    "        elif count >= 20 and total_reward > 22:\n",
    "            reward = 10\n",
    "        elif count >= 100 and total_reward < 50:\n",
    "            reward = -20\n",
    "        elif count >= 100 and total_reward >=90:\n",
    "            reward = 25\n",
    "        if done and reward < 1:\n",
    "            reward = -10\n",
    "        new_state = get_state_as_string(assign_bins(observation,bins))\n",
    "        y = Q.get(new_state)\n",
    "        if y == None:\n",
    "            new_states_created+=1\n",
    "            add_state_to_table(new_state,Q)\n",
    "        a1,max_q_s1a1 = q_states_dict(Q[new_state])\n",
    "        #Bellmans Equation\n",
    "        Q[state][act]= ((1 - ALPHA)*(Q[state][act])) + ALPHA*(reward + GAMMA * max_q_s1a1)\n",
    "        state,act = new_state, a1\n",
    "        if done:\n",
    "            q_t = Q\n",
    "    return total_reward, count,new_states_created,q_t\n",
    "\n",
    "def play_multiple(bins, N):\n",
    "    Q = initialise_q()\n",
    "    count=[]\n",
    "    new_state_array = 0\n",
    "    reward = []\n",
    "    for n in range(N):\n",
    "        if n > 1:\n",
    "            epsilon = 1/np.sqrt(n+1)\n",
    "        else:\n",
    "            epsilon = 0\n",
    "        epsilon = 1/np.sqrt(n+1)\n",
    "        ep_reward,ep_length, states_created,table = play_a_game(bins,Q,epsilon)\n",
    "        #print(\"ep_reward\",ep_reward,\"length\",ep_length)\n",
    "        new_state_array+=states_created\n",
    "        if n %100 == 0:\n",
    "            print(n,ep_reward,new_state_array)\n",
    "        count.append(ep_length)\n",
    "        reward.append(ep_reward)\n",
    "    return count,reward,table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = create_bins()  #create observation bins\n",
    "x,y,table = play_multiple(bins,1000) #observation bins, number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Learned values against environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range (20):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    print(\"-----------------------\")\n",
    "    #time.sleep(1)\n",
    "    count = 0\n",
    "    state = get_state_as_string(assign_bins(observation,bins))\n",
    "    while not done:\n",
    "        env.render()\n",
    "        y = table.get(state)\n",
    "        if y is None:\n",
    "            act = env.action_space.sample()\n",
    "        else:\n",
    "            act,_ = q_states_dict(table[state])\n",
    "        observation,reward,done,_ = env.step(act)\n",
    "        state = get_state_as_string(assign_bins(observation,bins))\n",
    "        count+=1\n",
    "        if done:\n",
    "            print(\"counter\", count)\n",
    "            break\n",
    "            \n",
    "        state = get_state_as_string(assign_bins(observation,bins))\n",
    "\n",
    "env.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
