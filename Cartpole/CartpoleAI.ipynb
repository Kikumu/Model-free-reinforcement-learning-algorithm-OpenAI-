{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----import libraries----------------------------------------#\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "import random as rand\n",
    "#-----set up environment------------------------------------#\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random weight generation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_moves = 0\n",
    "episode_length = []\n",
    "best_weights = np.zeros(4) #emmpty 4 digit array\n",
    "\n",
    "for i in range (100):\n",
    "    new_weights = [rand.uniform(0,1),rand.uniform(0,1),rand.uniform(0,1),rand.uniform(0,1)]\n",
    "    moves_length = []\n",
    "    for j in range(100):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        count = 0\n",
    "        while not done:\n",
    "            count+=1\n",
    "            action = 1 if np.dot(observation,new_weights)>0 else  0\n",
    "            observation,reward,done,_ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        moves_length.append(count)\n",
    "    avg_moves = float(sum(moves_length)/len(moves_length))\n",
    "    if avg_moves > best_moves:\n",
    "        best_moves = avg_moves\n",
    "        best_weights = new_weights\n",
    "    episode_length.append(avg_moves)\n",
    "    if i % 20 == 0:\n",
    "        print(\"best moves is \", best_moves)\n",
    "done = False\n",
    "counr = 0\n",
    "env = wrappers.Monitor(env, 'movefiles',force=True) #save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting a continuouss state into a discrete state\n",
    "MAX_STATES = 10**5\n",
    "GAMMA = 0.7   #discount factor\n",
    "ALPHA = 0.01 #Learning rate\n",
    "\n",
    "#q table disctionary lookup(like weights) gets state with highest reward?\n",
    "def q_states_dict(q):\n",
    "    max_q = float('-inf')\n",
    "    for key,val in q.items():\n",
    "        if val > max_q:\n",
    "            max_q = val\n",
    "            max_key = key\n",
    "    return max_key,max_q\n",
    "\n",
    "# our inputs\n",
    "def create_bins():\n",
    "    bins = np.zeros((4,10))\n",
    "    bins[0] = np.linspace(-4.8,4.8,10)\n",
    "    bins[1] = np.linspace(-10,10,10)\n",
    "    bins[2] = np.linspace(-41.8,41.8,10)\n",
    "    bins[3] = np.linspace(-10,10,10)\n",
    "    return bins\n",
    "#digitize bins since we are converting a continuous state to a discrete state\n",
    "\n",
    "def assign_bins(observation,bins):\n",
    "    env_state = np.zeros(4) #creates a single array to store the 4 states\n",
    "    for i in range(4):\n",
    "        env_state[i] = np.digitize(observation[i],bins[i])\n",
    "    return env_state\n",
    "\n",
    "def get_state_as_string(env_state):\n",
    "    env_string = ''.join(str(int(e)) for e in env_state)\n",
    "    return env_string\n",
    "\n",
    "def get_all_states_string():\n",
    "    env_states = []\n",
    "    for i in range(MAX_STATES):\n",
    "        env_states.append(str(i).zfill(4))\n",
    "    return env_states\n",
    "\n",
    "def initialise_q():\n",
    "    Q = {}\n",
    "    all_states = get_all_states_string()\n",
    "    for state in all_states:\n",
    "        Q[state] = {}\n",
    "        for action in range(env.action_space.n):\n",
    "            Q[state][action] = 0 #could be anything?....like a bias?...\n",
    "    return Q\n",
    "\n",
    "def play_a_game(bins,Q,eps=0.2):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    count = 0\n",
    "    state = get_state_as_string(assign_bins(observation,bins))\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        count+=1\n",
    "        if rand.uniform(0,1) > eps:\n",
    "            act = env.action_space.sample() #pick a random action\n",
    "        else:\n",
    "            act = q_states_dict(Q[state])[0]\n",
    "        observation,reward,done,_ = env.step(act)\n",
    "        total_reward+=reward\n",
    "        if done and count < 250:\n",
    "            reward = -400\n",
    "        new_state = get_state_as_string(assign_bins(observation,bins))\n",
    "        al,max_q_slal = q_states_dict(Q[new_state])\n",
    "        Q[state][act]+=ALPHA*(reward + GAMMA*max_q_slal - Q[state][act])\n",
    "        state,act = new_state, al\n",
    "    return total_reward, count\n",
    "\n",
    "def play_multiple(bins, N = 100000):\n",
    "    Q = initialise_q()\n",
    "    \n",
    "    count=[]\n",
    "    reward = []\n",
    "    for n in range(N):\n",
    "        epsilon = rand.uniform(0,1)\n",
    "        ep_reward,ep_length = play_a_game(bins,Q,epsilon)\n",
    "        if n % 200 == 0:\n",
    "            print(\"epsilon: \",epsilon, \"Reward: \", ep_reward)\n",
    "        count.append(ep_length)\n",
    "        reward.append(ep_reward)\n",
    "    return count,reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = create_bins()\n",
    "x,y = play_multiple(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
