{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----import libraries----------------------------------------#\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "import random as rand\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#-----set up environment------------------------------------#\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random weight generation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_moves = 0\n",
    "episode_length = []\n",
    "best_weights = np.zeros(4) #emmpty 4 digit array\n",
    "\n",
    "for i in range (100):\n",
    "    new_weights = [rand.uniform(0,1),rand.uniform(0,1),rand.uniform(0,1),rand.uniform(0,1)]\n",
    "    moves_length = []\n",
    "    for j in range(100):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        count = 0\n",
    "        while not done:\n",
    "            count+=1\n",
    "            action = 1 if np.dot(observation,new_weights)>0 else  0\n",
    "            observation,reward,done,_ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        moves_length.append(count)\n",
    "    avg_moves = float(sum(moves_length)/len(moves_length))\n",
    "    if avg_moves > best_moves:\n",
    "        best_moves = avg_moves\n",
    "        best_weights = new_weights\n",
    "    episode_length.append(avg_moves)\n",
    "    if i % 20 == 0:\n",
    "        print(\"best moves is \", best_moves)\n",
    "done = False\n",
    "counr = 0\n",
    "env = wrappers.Monitor(env, 'movefiles',force=True) #save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting a continuouss state into a discrete state\n",
    "MAX_STATES = 10**2\n",
    "GAMMA = 0.7   #discount factor\n",
    "ALPHA = 0.001 #Learning rate\n",
    "\n",
    "#q table disctionary lookup(like weights) gets state with highest reward?\n",
    "#q table disctionary lookup(like weights) gets state with highest reward?\n",
    "def q_states_dict(q):\n",
    "    a,b = max(q.items(), key=lambda k: k[1])\n",
    "    #print(\"z\",a,b)\n",
    "    return a,b\n",
    "# our inputs\n",
    "def create_bins():\n",
    "    bins = np.zeros((4,15))\n",
    "    bins[0] = np.linspace(-2.4,2.4,15)\n",
    "    bins[1] = np.linspace(-10,10,15)\n",
    "    bins[2] = np.linspace(-41.8,41.8,15)\n",
    "    bins[3] = np.linspace(-10,10,15)\n",
    "    return bins\n",
    "#digitize bins since we are converting a continuous state to a discrete state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_bins(observation,bins):\n",
    "    env_state = np.zeros(4) #creates a single array to store the 4 states\n",
    "    for i in range(4):\n",
    "        env_state[i] = np.digitize(observation[i],bins[i])\n",
    "    return env_state\n",
    "\n",
    "def get_state_as_string(env_state):\n",
    "    env_string = ''.join(str(int(e)) for e in env_state)\n",
    "    return env_string\n",
    "\n",
    "def get_all_states_string():\n",
    "    env_states = []\n",
    "    for i in range(MAX_STATES):\n",
    "        env_states.append(str(i).zfill(8))\n",
    "    return env_states\n",
    "\n",
    "def initialise_q():\n",
    "    Q = {}\n",
    "    all_states = get_all_states_string()\n",
    "    for state in all_states:\n",
    "        Q[state] = {}\n",
    "        for action in range(env.action_space.n):\n",
    "            Q[state][action] = 0 #set rewards\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_a_game(bins,Q,eps):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    count = 0\n",
    "    new_states_created = 0\n",
    "    total_reward = 0\n",
    "    state = get_state_as_string(assign_bins(observation,bins))\n",
    "    y = Q.get(state)\n",
    "    while not done:\n",
    "        #print(\"state\",state)\n",
    "        count+=1\n",
    "        init_random = np.random.uniform(0,1)\n",
    "        if init_random < eps:\n",
    "            act = env.action_space.sample() #pick a random action\n",
    "        else:\n",
    "            act,_ = q_states_dict(Q[state])\n",
    "        observation,reward,done,_ = env.step(act)\n",
    "        total_reward+=reward\n",
    "        if done and count < 200:\n",
    "            reward = -1000\n",
    "        new_state = get_state_as_string(assign_bins(observation,bins))\n",
    "        a1,max_q_s1a1 = q_states_dict(Q[new_state])\n",
    "        Q[state][act]= ((1 - ALPHA)*(Q[state][act])) + ALPHA*(reward + GAMMA * max_q_s1a1)\n",
    "        state,act = new_state, a1\n",
    "        if done:\n",
    "            q_t = Q\n",
    "    return total_reward, count,new_states_created,q_t\n",
    "\n",
    "def play_multiple(bins, N):\n",
    "    Q = initialise_q()\n",
    "    count=[]\n",
    "    new_state_array = 0\n",
    "    reward = []\n",
    "    for n in range(N):\n",
    "        if n > 1:\n",
    "            epsilon = 1/np.sqrt(n+1)\n",
    "        else:\n",
    "            epsilon = 0\n",
    "        epsilon = 1/np.sqrt(n+1)\n",
    "        ep_reward,ep_length, states_created,table = play_a_game(bins,Q,epsilon)\n",
    "        new_state_array+=states_created\n",
    "        if n %10000 == 0:\n",
    "            print(n,epsilon,ep_reward,new_state_array)\n",
    "        count.append(ep_length)\n",
    "        reward.append(ep_reward)\n",
    "    return count,reward,table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = create_bins()\n",
    "x,y,table = play_multiple(bins,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table\n",
    "import pandas as pd\n",
    "q_tdf =  pd.DataFrame(table)\n",
    "q_tdf.to_csv(\"Q_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_array = []\n",
    "for i in range(len(y)):\n",
    "    y_array.append(i)\n",
    "y1 = y\n",
    "x1 = y_array\n",
    "plt.plot(x1,y,\"b\") #actual\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(y)\n",
    "r_avg = np.empty(N)\n",
    "for t in range(N):\n",
    "    r_avg[t] = np.mean(y[max(0,t-100):(t+1)])\n",
    "plt.plot(r_avg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range (20):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    print(\"-----------------------\")\n",
    "    #time.sleep(1)\n",
    "    count = 0\n",
    "    state = get_state_as_string(assign_bins(observation,bins))\n",
    "    while not done:\n",
    "        env.render()\n",
    "        act,_ = q_states_dict(table[state])\n",
    "        observation,reward,done,_ = env.step(act)\n",
    "        state = get_state_as_string(assign_bins(observation,bins))\n",
    "        count+=1\n",
    "        if done:\n",
    "            print(\"counter\", count)\n",
    "            break\n",
    "            \n",
    "        state = get_state_as_string(assign_bins(observation,bins))\n",
    "\n",
    "env.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
