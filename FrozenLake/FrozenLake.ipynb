{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessarry Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "import random as rand\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output \n",
    "#-----set up environment------------------------------------#\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STATES = 15\n",
    "GAMMA = 0.8\n",
    "ALPHA =0.01\n",
    "\n",
    "def get_state_as_string(env_state):\n",
    "    env_string = ''.join(str(int(e)) for e in env_state)\n",
    "    return env_string\n",
    "\n",
    "def get_all_states_string():\n",
    "    env_states = []\n",
    "    for i in range(MAX_STATES):\n",
    "        env_states.append(str(i).zfill(0))\n",
    "    return env_states\n",
    "\n",
    "def initialise_q():\n",
    "    Q = {}\n",
    "    all_states = get_all_states_string()\n",
    "    for state in all_states:\n",
    "        Q[state] = {}\n",
    "        for action in range(env.action_space.n):\n",
    "            Q[state][action] = 0 #set rewards\n",
    "    return Q\n",
    "\n",
    "def add_state_to_table(new_state,table):\n",
    "    n_value = {}\n",
    "    n_value[new_state] = {}\n",
    "    for action in range(env.action_space.n):\n",
    "        n_value[new_state][action] = 0\n",
    "    table.update(n_value)\n",
    "\n",
    "def q_states_dict(q):\n",
    "    a,b = max(q.items(), key=lambda k: k[1])\n",
    "    #print(\"z\",a,b)\n",
    "    return a,b\n",
    "\n",
    "def play_a_game(Q,exp_rate):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    count = 0\n",
    "    new_states_created = 0\n",
    "    total_reward = 0\n",
    "    state = str(observation)\n",
    "    y = Q.get(state)\n",
    "    if y == None:\n",
    "        add_state_to_table(state,Q)\n",
    "        new_states_created+=1\n",
    "    while not done:\n",
    "        count+=1\n",
    "        init_random = np.random.uniform(0,1)\n",
    "        exp_thresh = rand.uniform(0,1)\n",
    "        if exp_thresh > exp_rate:\n",
    "            act,_ = q_states_dict(Q[state])\n",
    "        else:\n",
    "            act = env.action_space.sample()\n",
    "        observation,reward,done,_ = env.step(act)\n",
    "        total_reward+=reward\n",
    "        new_state = str(observation)\n",
    "        y = Q.get(new_state)\n",
    "        if y == None:\n",
    "            new_states_created+=1\n",
    "            add_state_to_table(new_state,Q)\n",
    "        a1,max_q_s1a1 = q_states_dict(Q[new_state])\n",
    "        Q[state][act]= ((1 - ALPHA)*(Q[state][act])) + ALPHA*(reward + GAMMA * max_q_s1a1)\n",
    "        #state,act = new_state,a1\n",
    "        if done:\n",
    "            if reward == 0:\n",
    "                reward = -0.0625\n",
    "                Q[state][act]= ((1 - ALPHA)*(Q[state][act])) + ALPHA*(reward + GAMMA * max_q_s1a1)\n",
    "            elif reward == 1:\n",
    "                reward = 1+0.0625\n",
    "                Q[state][act]= ((1 - ALPHA)*(Q[state][act])) + ALPHA*(reward + GAMMA * max_q_s1a1)\n",
    "            q_t = Q\n",
    "            break\n",
    "        state,act = new_state,a1\n",
    "    return total_reward, count,new_states_created,q_t\n",
    "\n",
    "def play_multiple(N):\n",
    "    Q = initialise_q()\n",
    "    count=[]\n",
    "    new_state_array = 0\n",
    "    reward = []\n",
    "    exp_rate = 1\n",
    "    episode = 0\n",
    "    max_rate = 1\n",
    "    min_rate = 0.01\n",
    "    exp_decay = 0.000001\n",
    "    for n in range(N):\n",
    "        ep_reward,ep_length,states_created,q_t = play_a_game(Q,exp_rate)\n",
    "        episode+=1\n",
    "        exp_rate = min_rate + (max_rate - min_rate)*np.exp(-exp_decay * episode)\n",
    "        new_state_array+=states_created\n",
    "        if n %10000 == 0:\n",
    "            print(n,ep_reward,new_state_array)\n",
    "        count.append(ep_length)\n",
    "        reward.append(ep_reward)\n",
    "    return count,reward,q_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 0\n",
      "10000 0.0 1\n",
      "20000 0.0 1\n",
      "30000 0.0 1\n",
      "40000 0.0 1\n",
      "50000 0.0 1\n",
      "60000 0.0 1\n",
      "70000 0.0 1\n",
      "80000 0.0 1\n",
      "90000 0.0 1\n"
     ]
    }
   ],
   "source": [
    "x,y,q_t = play_multiple(100000)#number of games wished to play/train on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Q table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1e6fd82b82a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mq_t\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'q_t' is not defined"
     ]
    }
   ],
   "source": [
    "q_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1558.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Learned values against environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins:  3  loses:  0\n"
     ]
    }
   ],
   "source": [
    "wins = 0\n",
    "loses = 0\n",
    "numberof games = 3\n",
    "for episode in range (3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"-----------------------\")\n",
    "    time.sleep(1)\n",
    "    for step in range(150):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        state = str(state)\n",
    "        act,_ = q_states_dict(q_t[state])\n",
    "        new_sts,reward,done,_ = env.step(act)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"Found Goal!\")\n",
    "                wins+=1\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"You Lose\")\n",
    "                loses+=1\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        state = new_sts\n",
    "\n",
    "print(\"wins: \",wins,\" loses: \",loses)\n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay memory is finite\n",
    "#poicy network\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
